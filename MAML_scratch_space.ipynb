{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cv(length):\n",
    "    if length == 0:\n",
    "        return [\"\"]\n",
    "    \n",
    "    else:\n",
    "        previous = generate_cv(length - 1)\n",
    "        new = []\n",
    "        \n",
    "        for elt in previous:\n",
    "            new.append(elt + \"V\")\n",
    "            new.append(elt + \"C\")\n",
    "            \n",
    "        return new\n",
    "    \n",
    "def generate_cv_cumul(max_length):\n",
    "    output = []\n",
    "    for i in range(max_length + 1):\n",
    "        output += generate_cv(i)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabifiable(word):\n",
    "    if word[:2] == \"CC\":\n",
    "        return False\n",
    "    if word[-2:] == \"CC\":\n",
    "        return False\n",
    "    if \"CCC\" in word:\n",
    "        return False\n",
    "    if word == \"C\":\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def syllabify(word):\n",
    "    prev = \"#\"\n",
    "    syll = \"\"\n",
    "    \n",
    "    rev = word[::-1]\n",
    "    \n",
    "    for char in rev:\n",
    "        if prev == \"#\":\n",
    "            syll += \".\"\n",
    "            syll += char\n",
    "            prev = char\n",
    "        elif prev == \"C\" and char == \"V\":\n",
    "            syll += char\n",
    "            prev = char\n",
    "        elif prev == \"V\" and char == \"C\":\n",
    "            syll += char\n",
    "            syll += \".\"\n",
    "            prev = \".\"\n",
    "        elif prev == \"V\" and char == \"V\":\n",
    "            syll += \".\"\n",
    "            syll += char\n",
    "            prev = char\n",
    "        elif prev == \".\":\n",
    "            syll += char\n",
    "            prev = char\n",
    "        else:\n",
    "            print(word)\n",
    "    \n",
    "    syll = syll[::-1]\n",
    "    if len(syll) > 0:\n",
    "        if syll[0] == \"V\" or syll[0] == \"C\":\n",
    "            syll = \".\" + syll\n",
    "    \n",
    "    return syll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violations(ur, sr):\n",
    "    onset = 0\n",
    "    nocoda = 0\n",
    "    mx = 0\n",
    "    dep = 0\n",
    "    \n",
    "    if len(sr) > 0:\n",
    "        if sr[0] == \".\":\n",
    "            sr = sr[1:]\n",
    "        if sr[-1] == \".\":\n",
    "            sr = sr[:-1]\n",
    "        \n",
    "        syllables = sr.split(\".\")\n",
    "        \n",
    "\n",
    "        # Onset, NoCoda\n",
    "        for syllable in syllables:\n",
    "            parts = syllable.split(\"V\")\n",
    "            ons = parts[0]\n",
    "            cod = parts[1]\n",
    "            if ons == \"\":\n",
    "                onset += 1\n",
    "            if cod != \"\":\n",
    "                nocoda += 1\n",
    "            \n",
    "    # Max, Dep\n",
    "    edit_paths = edit_path(ur,sr.replace(\".\",\"\"))\n",
    "    \n",
    "    all_violations = []\n",
    "    for path in edit_paths:\n",
    "        all_violations.append([onset, nocoda, path[1], path[0]])\n",
    "        \n",
    "    return all_violations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_path(w1,w2):\n",
    "    l1 = len(w1) + 1\n",
    "    l2 = len(w2) + 1\n",
    "    \n",
    "    #if l1 == 0 and l2 == 0:\n",
    "    #    return [[0,0]]\n",
    "    #elif l1 == 0:\n",
    "    #    return [[l2, 0]]\n",
    "    #elif l2 == 0:\n",
    "    #    return [[0, l2]]\n",
    "    \n",
    "    grid = [[0 for i in range(l2)] for j in range(l1)]\n",
    "    \n",
    "    for ind in range(l1):\n",
    "        grid[ind][0] = [[0,ind]]\n",
    "    for ind in range(l2):\n",
    "        grid[0][ind] = [[ind,0]]\n",
    "        \n",
    "    #print(grid)\n",
    "        \n",
    "    for i1 in range(1,l1):\n",
    "        for i2 in range(1,l2):\n",
    "            p1 = grid[i1-1][i2]\n",
    "            p2 = grid[i1][i2-1]\n",
    "            \n",
    "            if w1[i1-1] == w2[i2-1]:\n",
    "                possibles = grid[i1-1][i2-1]\n",
    "            else:\n",
    "                new_poss = []\n",
    "                \n",
    "                possibles = p1 \n",
    "                for poss in possibles:\n",
    "                    new_poss.append([poss[0], 1 + poss[1]])\n",
    "                    \n",
    "                possibles = p2 \n",
    "                for poss in possibles:\n",
    "                    new_poss.append([1 + poss[0], poss[1]])\n",
    "                    \n",
    "                possibles = new_poss\n",
    "            \n",
    "            grid[i1][i2] = min_cands(possibles)\n",
    "            \n",
    "    return grid[l1-1][l2-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cands(cands):\n",
    "    min_first = 1000000\n",
    "    min_second = 1000000\n",
    "    \n",
    "    for cand in cands:\n",
    "        first = cand[0]\n",
    "        second = cand[1]\n",
    "        \n",
    "        if first < min_first:\n",
    "            min_first = first\n",
    "            \n",
    "        if second < min_second:\n",
    "            min_second = second\n",
    "            \n",
    "    firsts = []\n",
    "    seconds = []\n",
    "    \n",
    "    for cand in cands:\n",
    "        if cand[0] == min_first:\n",
    "            firsts.append(cand)\n",
    "        if cand[1] == min_second:\n",
    "            seconds.append(cand)\n",
    "            \n",
    "    min_second_firsts = 1000000\n",
    "    best_first = []\n",
    "    for first in firsts:\n",
    "        if first[1] < min_second_firsts:\n",
    "            best_first = first\n",
    "            \n",
    "    min_first_seconds = 1000000\n",
    "    best_second = []\n",
    "    for second in seconds:\n",
    "        if second[1] < min_first_seconds:\n",
    "            best_second = second\n",
    "            \n",
    "    if best_first[0] ==  best_second[0] and best_first[1] == best_second[1]:\n",
    "        return [best_first]\n",
    "    else:\n",
    "        return [best_first, best_second]\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner(ur, candidates, ranking):\n",
    "    all_violations = []\n",
    "    for cand in candidates:\n",
    "        viols = violations(ur, cand)\n",
    "        for viol in viols:\n",
    "            all_violations += [[cand, viol]] \n",
    "            \n",
    "    for constraint in ranking:\n",
    "        min_viols = 1000000\n",
    "        for candidate in all_violations:\n",
    "            #print(all_violations, candidate)\n",
    "            this_constraint_viols = candidate[1][constraint]\n",
    "            if this_constraint_viols < min_viols:\n",
    "                min_viols = this_constraint_viols\n",
    "                \n",
    "        filtered_cands = []\n",
    "        for candidate in all_violations:\n",
    "            if candidate[1][constraint] == min_viols:\n",
    "                filtered_cands.append(candidate)\n",
    "                \n",
    "        all_violations = filtered_cands\n",
    "        \n",
    "    return all_violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_cv_cumul(5)\n",
    "outputs = []\n",
    "\n",
    "for inp in generate_cv_cumul(10):\n",
    "    if syllabifiable(inp):\n",
    "        outputs.append(syllabify(inp))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rankings = [\n",
    "    [0,1,2,3],\n",
    "    [0,1,3,2],\n",
    "    [0,2,3,1],\n",
    "    [0,3,2,1],\n",
    "    [2,3,0,1],\n",
    "    [3,2,0,1],\n",
    "    [1,2,3,0], # withheld\n",
    "    [1,3,2,0] # withheld\n",
    "]\n",
    "\n",
    "test_rankings = [\n",
    "]\n",
    "\n",
    "all_rankings = train_rankings + test_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[0, 1, 3, 2]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[0, 2, 3, 1]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[0, 3, 2, 1]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[2, 3, 0, 1]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[3, 2, 0, 1]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[1, 2, 3, 0]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n",
      "[1, 3, 2, 0]\n",
      "\n",
      "V\n",
      "C\n",
      "VV\n",
      "VC\n",
      "CV\n",
      "CC\n",
      "VVV\n",
      "VVC\n",
      "VCV\n",
      "VCC\n",
      "CVV\n",
      "CVC\n",
      "CCV\n",
      "CCC\n",
      "VVVV\n",
      "VVVC\n",
      "VVCV\n",
      "VVCC\n",
      "VCVV\n",
      "VCVC\n",
      "VCCV\n",
      "VCCC\n",
      "CVVV\n",
      "CVVC\n",
      "CVCV\n",
      "CVCC\n",
      "CCVV\n",
      "CCVC\n",
      "CCCV\n",
      "CCCC\n",
      "VVVVV\n",
      "VVVVC\n",
      "VVVCV\n",
      "VVVCC\n",
      "VVCVV\n",
      "VVCVC\n",
      "VVCCV\n",
      "VVCCC\n",
      "VCVVV\n",
      "VCVVC\n",
      "VCVCV\n",
      "VCVCC\n",
      "VCCVV\n",
      "VCCVC\n",
      "VCCCV\n",
      "VCCCC\n",
      "CVVVV\n",
      "CVVVC\n",
      "CVVCV\n",
      "CVVCC\n",
      "CVCVV\n",
      "CVCVC\n",
      "CVCCV\n",
      "CVCCC\n",
      "CCVVV\n",
      "CCVVC\n",
      "CCVCV\n",
      "CCVCC\n",
      "CCCVV\n",
      "CCCVC\n",
      "CCCCV\n",
      "CCCCC\n"
     ]
    }
   ],
   "source": [
    "all_input_outputs = {}\n",
    "\n",
    "for ranking in all_rankings:\n",
    "    print(ranking)\n",
    "    io_list = []\n",
    "    \n",
    "    for inp in inputs:\n",
    "        print(inp)\n",
    "        output = winner(inp, outputs, ranking)[0][0]\n",
    "        io_list.append([inp, output])\n",
    "    \n",
    "    all_input_outputs[tuple(ranking)] = io_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_task(ranking, all_input_outputs, n=10):\n",
    "    io_list = all_input_outputs[tuple(ranking)][:]\n",
    "    shuffle(io_list)\n",
    "    \n",
    "    train_pairs = io_list[:n]\n",
    "    test_pairs = io_list[n:]\n",
    "        \n",
    "    return train_pairs, test_pairs, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_dict = {}\n",
    "\n",
    "for key in all_input_outputs:\n",
    "    interior_dict = {}\n",
    "    \n",
    "    io_list = all_input_outputs[key]\n",
    "    \n",
    "    for elt in io_list:\n",
    "        interior_dict[elt[0]] = elt[1]\n",
    "        \n",
    "    io_dict[key] = interior_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_preds_dict(inputs, r1, r2):\n",
    "    for inp in inputs:\n",
    "        p1 = io_dict[tuple(r1)][inp]\n",
    "        p2 = io_dict[tuple(r2)][inp]\n",
    "        \n",
    "        if p1 != p2:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_task_dict(task, rankings):\n",
    "    inputs = []\n",
    "    for pair in task[0]:\n",
    "        inputs.append(pair[0])\n",
    "        \n",
    "    for i1 in range(len(rankings)):\n",
    "        for i2 in range(i1+1, len(rankings)):\n",
    "            r1 = rankings[i1]\n",
    "            r2 = rankings[i2]\n",
    "    \n",
    "            if same_preds_dict(inputs, r1, r2):\n",
    "                return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "\n",
    "n_train = 20\n",
    "\n",
    "while len(train_set) < 20000:\n",
    "    ranking = random.choice(train_rankings)\n",
    "    task = make_task(ranking, all_input_outputs, n=n_train)\n",
    "    \n",
    "    if check_task_dict(task, all_rankings):\n",
    "        train_set.append(task)\n",
    "        \n",
    "        if len(train_set) % 1000 == 0:\n",
    "            print(len(train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['VCCCV', '.V.CV.'],\n",
       "  ['CVVC', '.CV.V.'],\n",
       "  ['VCVCC', '.V.CV.'],\n",
       "  ['VVVVV', '.V.V.V.V.V.'],\n",
       "  ['VVCVC', '.V.V.CV.'],\n",
       "  ['CVCCV', '.CV.CV.'],\n",
       "  ['VVVVC', '.V.V.V.V.'],\n",
       "  ['CCVVV', '.CV.V.V.'],\n",
       "  ['V', '.V.'],\n",
       "  ['VCVVV', '.V.CV.V.V.'],\n",
       "  ['VCCC', '.V.'],\n",
       "  ['VCCVC', '.V.CV.'],\n",
       "  ['CCV', '.CV.'],\n",
       "  ['CCVCC', '.CV.'],\n",
       "  ['', ''],\n",
       "  ['CVCC', '.CV.'],\n",
       "  ['VC', '.V.'],\n",
       "  ['CC', ''],\n",
       "  ['CVCCC', '.CV.'],\n",
       "  ['VCV', '.V.CV.']],\n",
       " [['CVC', '.CV.'],\n",
       "  ['CVCVV', '.CV.CV.V.'],\n",
       "  ['VCCVV', '.V.CV.V.'],\n",
       "  ['VVC', '.V.V.'],\n",
       "  ['CVCVC', '.CV.CV.'],\n",
       "  ['VCVVC', '.V.CV.V.'],\n",
       "  ['CCVVC', '.CV.V.'],\n",
       "  ['CCCVC', '.CV.'],\n",
       "  ['VVCCC', '.V.V.'],\n",
       "  ['VVCVV', '.V.V.CV.V.'],\n",
       "  ['CCCC', ''],\n",
       "  ['CCC', ''],\n",
       "  ['VVVCC', '.V.V.V.'],\n",
       "  ['CVVCC', '.CV.V.'],\n",
       "  ['CCCV', '.CV.'],\n",
       "  ['VCCCC', '.V.'],\n",
       "  ['CVVCV', '.CV.V.CV.'],\n",
       "  ['CCVC', '.CV.'],\n",
       "  ['CVCV', '.CV.CV.'],\n",
       "  ['VV', '.V.V.'],\n",
       "  ['CCCVV', '.CV.V.'],\n",
       "  ['CVVVC', '.CV.V.V.'],\n",
       "  ['VCVCV', '.V.CV.CV.'],\n",
       "  ['CCCCV', '.CV.'],\n",
       "  ['VVCV', '.V.V.CV.'],\n",
       "  ['CCVV', '.CV.V.'],\n",
       "  ['VVVCV', '.V.V.V.CV.'],\n",
       "  ['CVVVV', '.CV.V.V.V.'],\n",
       "  ['VVVV', '.V.V.V.V.'],\n",
       "  ['VCCV', '.V.CV.'],\n",
       "  ['VCC', '.V.'],\n",
       "  ['CVVV', '.CV.V.V.'],\n",
       "  ['CVV', '.CV.V.'],\n",
       "  ['VVV', '.V.V.V.'],\n",
       "  ['C', ''],\n",
       "  ['CCVCV', '.CV.CV.'],\n",
       "  ['VCVV', '.V.CV.V.'],\n",
       "  ['VVVC', '.V.V.V.'],\n",
       "  ['VCVC', '.V.CV.'],\n",
       "  ['VVCCV', '.V.V.CV.'],\n",
       "  ['VVCC', '.V.V.'],\n",
       "  ['CCCCC', ''],\n",
       "  ['CV', '.CV.']],\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from models import *\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle([3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_dataset(\"phonology.train\")\n",
    "dev_set = load_dataset(\"phonology.dev\")\n",
    "test_set = load_dataset(\"phonology.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.enc_vocab_size = enc_vocab_size\n",
    "        self.dec_vocab_size = dec_vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.enc_embedding = GradEmbedding(enc_vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_embedding = GradEmbedding(dec_vocab_size, input_size)\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, dec_vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "\n",
    "        self.char2ind = {}\n",
    "        self.ind2char = {}\n",
    "\n",
    "    def forward(self, sequence_list):\n",
    "        # Initialize the hidden state\n",
    "        hidden = (V(torch.zeros(1, len(sequence_list), self.hidden_size)),\n",
    "                  V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "\n",
    "        all_seqs = []\n",
    "        for sequence in sequence_list:\n",
    "            this_seq = []\n",
    "            # Iterate over the sequence\n",
    "            for elt in sequence:\n",
    "                ind = self.char2ind[elt]\n",
    "                this_seq.append(ind)\n",
    "            all_seqs.append(torch.LongTensor(this_seq))\n",
    "            \n",
    "        #print(all_seqs)\n",
    "        #print(torch.nn.utils.rnn.pad_sequence(all_seqs))\n",
    "        #all_seqs = torch.LongTensor(all_seqs).transpose(0,1)\n",
    "        all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "        all_seqs_onehot = (all_seqs > 0).type(torch.FloatTensor)\n",
    "        #print(all_seqs_onehot)\n",
    "        #all_seqs = torch.nn.utils.rnn.pack_padded_sequence(all_seqs, lengths=[len(x) for x in sequence_list])\n",
    "                \n",
    "        #print(all_seqs)\n",
    "        #print(all_seqs[0])\n",
    "        #print(all_seqs.shape)\n",
    "            \n",
    "        for index, elt in enumerate(all_seqs):\n",
    "            #print(elt)\n",
    "            emb = self.enc_embedding(elt.unsqueeze(0))\n",
    "            #print(emb.shape, hidden[0].shape, hidden[1].shape)\n",
    "            output, hidden_new = self.enc_lstm(emb, hidden)\n",
    "            hidden_prev = hidden\n",
    "            #print(hidden_prev[0].shape)\n",
    "            #print(all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape))\n",
    "            hx = hidden_prev[0] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)) + hidden_new[0] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)\n",
    "            cx = hidden_prev[1] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)) + hidden_new[1] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)\n",
    "            \n",
    "            hidden = (hx, cx)\n",
    "            #print(\"\")\n",
    "            #print(hidden)\n",
    "            \n",
    "        #print(hidden[0].shape, hidden[1].shape)\n",
    "            \n",
    "\n",
    "        # Return the final hidden state\n",
    "        # Note that hidden = (hidden state, cell state). So hidden[0] is just the hidden state\n",
    "        #encoding = hidden[0]\n",
    "        #print(encoding.shape)\n",
    "\n",
    "        #hidden = (encoding,\n",
    "        #          V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "\n",
    "        prev_output = [\"SOS\" for _ in range(len(sequence_list))]\n",
    "        out_strings = [\"\" for _ in range(len(sequence_list))]\n",
    "        logits = []\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            prev_outputs = []\n",
    "            for elt in prev_output:\n",
    "                ind = self.char2ind[elt]\n",
    "                prev_outputs.append(ind)\n",
    "                \n",
    "            emb = self.dec_embedding(torch.LongTensor([prev_outputs]))\n",
    "            output, hidden = self.dec_lstm(emb, hidden)\n",
    "            pred = self.dec_output(output)\n",
    "\n",
    "            probs = F.log_softmax(pred, dim=2)\n",
    "            logits.append(probs)\n",
    "\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            label = topi[0] #.item()\n",
    "            \n",
    "            prev_output = []\n",
    "            for index, elt in enumerate(label):\n",
    "                char = self.ind2char[elt.item()]\n",
    "                \n",
    "                out_strings[index] += char\n",
    "                prev_output.append(char)\n",
    "\n",
    "            #char = self.ind2char[label]\n",
    "\n",
    "\n",
    "            #if char == \"EOS\":\n",
    "            #    break\n",
    "\n",
    "            #out_string += char\n",
    "\n",
    "            #prev_output = char\n",
    "\n",
    "        # Return the final hidden state\n",
    "        # Note that hidden = (hidden state, cell state). So hidden[0] is just the hidden state\n",
    "        return out_strings, logits\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return [('enc_embedding', self.enc_embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_embedding', self.dec_embedding), ('dec_lstm', self.dec_lstm),\n",
    "                ('dec_output', self.dec_output)]\n",
    "\n",
    "    def set_dict(self,v_list,c_list):\n",
    "        char2ind = {}\n",
    "        char2ind[\"NULL\"] = 0\n",
    "        char2ind[\"SOS\"] = 1\n",
    "        char2ind[\"EOS\"] = 2\n",
    "        char2ind[\".\"] = 3\n",
    "        char2ind[\"C\"] = 4\n",
    "        char2ind[\"V\"] = 5\n",
    "        #char2ind[\"a\"] = 4\n",
    "        #char2ind[\"b\"] = 5\n",
    "        char2ind[\"c\"] = 6\n",
    "        char2ind[\"d\"] = 7\n",
    "        char2ind[\"e\"] = 8\n",
    "        char2ind[\"f\"] = 9\n",
    "        char2ind[\"g\"] = 10\n",
    "        char2ind[\"h\"] = 11\n",
    "        char2ind[\"i\"] = 12\n",
    "        char2ind[\"j\"] = 13\n",
    "        char2ind[\"k\"] = 14\n",
    "        char2ind[\"l\"] = 15\n",
    "        char2ind[\"m\"] = 16\n",
    "        char2ind[\"n\"] = 17\n",
    "        char2ind[\"o\"] = 18\n",
    "        char2ind[\"p\"] = 19\n",
    "        char2ind[\"q\"] = 20\n",
    "        char2ind[\"r\"] = 21\n",
    "        char2ind[\"s\"] = 22\n",
    "        char2ind[\"t\"] = 23\n",
    "        char2ind[\"u\"] = 24\n",
    "        char2ind[\"v\"] = 25\n",
    "        char2ind[\"w\"] = 26\n",
    "        char2ind[\"x\"] = 27\n",
    "        char2ind[\"y\"] = 28\n",
    "        char2ind[\"z\"] = 29\n",
    "        char2ind[\"A\"] = 30\n",
    "        char2ind[\"E\"] = 31\n",
    "        char2ind[\"I\"] = 32\n",
    "        char2ind[\"O\"] = 33\n",
    "        char2ind[\"U\"] = 34\n",
    "\n",
    "        ind2char = {}\n",
    "        ind2char[0] = \"NULL\"\n",
    "        ind2char[1] = \"SOS\"\n",
    "        ind2char[2] = \"EOS\"\n",
    "        ind2char[3] = \".\"\n",
    "        ind2char[4] = \"C\"\n",
    "        ind2char[5] = \"V\"\n",
    "        \n",
    "        #ind2char[4] = \"a\"\n",
    "        #ind2char[5] = \"b\"\n",
    "        ind2char[6] = \"c\"\n",
    "        ind2char[7] = \"d\"\n",
    "        ind2char[8] = \"e\"\n",
    "        ind2char[9] = \"f\"\n",
    "        ind2char[10] = \"g\"\n",
    "        ind2char[11] = \"h\"\n",
    "        ind2char[12] = \"i\"\n",
    "        ind2char[13] = \"j\"\n",
    "        ind2char[14] = \"k\"\n",
    "        ind2char[15] = \"l\"\n",
    "        ind2char[16] = \"m\"\n",
    "        ind2char[17] = \"n\"\n",
    "        ind2char[18] = \"o\"\n",
    "        ind2char[19] = \"p\"\n",
    "        ind2char[20] = \"q\"\n",
    "        ind2char[21] = \"r\"\n",
    "        ind2char[22] = \"s\"\n",
    "        ind2char[23] = \"t\"\n",
    "        ind2char[24] = \"u\"\n",
    "        ind2char[25] = \"v\"\n",
    "        ind2char[26] = \"w\"\n",
    "        ind2char[27] = \"x\"\n",
    "        ind2char[28] = \"y\"\n",
    "        ind2char[29] = \"z\"\n",
    "        ind2char[30] = \"A\"\n",
    "        ind2char[31] = \"E\"\n",
    "        ind2char[32] = \"I\"\n",
    "        ind2char[33] = \"O\"\n",
    "        ind2char[34] = \"U\"\n",
    "\n",
    "        #phoneme_list = v_list + c_list\n",
    "        #possible_indices = [i for i in range(4,12)]\n",
    "        #shuffle(possible_indices)\n",
    "\n",
    "        #indices = possible_indices[:len(phoneme_list)]\n",
    "        #remainder = possible_indices[len(phoneme_list):]\n",
    "\n",
    "        #for i in range(len(phoneme_list)):\n",
    "        #    char2ind[phoneme_list[i]] = indices[i]\n",
    "        #    ind2char[indices[i]] = phoneme_list[i]\n",
    "\n",
    "        #for ind in remainder:\n",
    "        #    char2ind[str(ind)] = ind\n",
    "        #    ind2char[ind] = str(ind)\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_REPTILE = EncoderDecoder(6,6,20,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_REPTILE.set_dict(train_set[0][2], train_set[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['VCCCV', '.V.CV.'], ['CVVC', '.CV.V.'], ['VCVCC', '.V.CV.']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOSSOS'],\n",
       " [tensor([[[-1.7259, -1.6470, -1.8447, -1.8279, -1.8552, -1.8701]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7548, -1.6605, -1.8376, -1.7924, -1.8412, -1.8798]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7701, -1.6678, -1.8338, -1.7753, -1.8349, -1.8829]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7780, -1.6715, -1.8320, -1.7667, -1.8321, -1.8839]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7821, -1.6732, -1.8313, -1.7623, -1.8308, -1.8843]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7843, -1.6739, -1.8310, -1.7601, -1.8302, -1.8845]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7856, -1.6740, -1.8310, -1.7589, -1.8299, -1.8846]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7863, -1.6740, -1.8309, -1.7583, -1.8298, -1.8848]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7867, -1.6738, -1.8309, -1.7580, -1.8297, -1.8849]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7869, -1.6737, -1.8309, -1.7579, -1.8297, -1.8850]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7871, -1.6735, -1.8308, -1.7579, -1.8297, -1.8851]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7872, -1.6734, -1.8308, -1.7578, -1.8297, -1.8851]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7873, -1.6734, -1.8308, -1.7578, -1.8297, -1.8852]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7873, -1.6733, -1.8308, -1.7578, -1.8297, -1.8852]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7873, -1.6733, -1.8308, -1.7578, -1.8298, -1.8852]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7873, -1.6732, -1.8308, -1.7578, -1.8298, -1.8852]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7873, -1.6732, -1.8307, -1.7578, -1.8298, -1.8852]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7874, -1.6732, -1.8307, -1.7578, -1.8298, -1.8853]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7874, -1.6732, -1.8307, -1.7578, -1.8298, -1.8853]]],\n",
       "         grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([[[-1.7874, -1.6732, -1.8307, -1.7578, -1.8298, -1.8853]]],\n",
       "         grad_fn=<LogSoftmaxBackward>)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_REPTILE(['VCCCV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"dogEOS\"[:\"dogEOS\".index(\"EOS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"dogEOS\".index(\"EOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    if \"EOS\" in output:\n",
    "        return output[:output.index(\"EOS\")]\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fit_task(model, task, lr_inner, create_graph=True):\n",
    "    training_set = task[0]\n",
    "    test_set = task[1]\n",
    "    v_list = task[2]\n",
    "    c_list = task[3]\n",
    "    \n",
    "    new_model = EncoderDecoder(model.enc_vocab_size,model.dec_vocab_size,model.input_size,model.hidden_size)\n",
    "    new_model.copy(model, same_var=True)\n",
    "    #new_model.copy(model, same_var=False)\n",
    "    \n",
    "    new_model.set_dict(v_list, c_list)\n",
    "    \n",
    "    loss = 0\n",
    "    criterion = nn.NLLLoss(ignore_index=0, size_average=False)\n",
    "\n",
    "    output, logits = new_model([pair[0] for pair in training_set])\n",
    "    \n",
    "    all_seqs = [] # HERE\n",
    "    for inp, sequence in training_set:\n",
    "        this_seq = []\n",
    "        # Iterate over the sequencex\n",
    "        for elt in sequence:\n",
    "            ind = new_model.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        this_seq.append(new_model.char2ind[\"EOS\"])\n",
    "        all_seqs.append(torch.LongTensor(this_seq))\n",
    "            \n",
    "    all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "    #print(all_seqs)\n",
    "\n",
    "    for index, logit in enumerate(logits):\n",
    "        if index >= len(all_seqs):\n",
    "            break\n",
    "            \n",
    "        loss += criterion(logit[0], all_seqs[index])\n",
    "        \n",
    "            \n",
    "    total_values = sum([len(x[1]) + 1 for x in training_set])  \n",
    "    loss /= total_values\n",
    "    loss.backward(create_graph=create_graph, retain_graph=True)\n",
    "    \n",
    "    #print(\"\")\n",
    "    #print(\"INNER UPDATE\")\n",
    "    #print(new_model.enc_lstm.wi_bias)\n",
    "    #for param in new_model.params():\n",
    "    #    print(param[0])\n",
    "        #print(\"\")\n",
    "   #     break\n",
    "    for name, param in new_model.named_params():\n",
    "        grad = param.grad\n",
    "   #     print(\"grad\", grad)\n",
    "   #     print(param)\n",
    "   #     print(param - lr_inner * grad)\n",
    "                \n",
    "        new_model.set_param(name, param - lr_inner * grad)\n",
    "    #print(new_model.enc_lstm.wi_bias)\n",
    "    #for param in new_model.params():\n",
    "    #    print(param[0])\n",
    "    #    print(\"DONE INNER UPDATE\")\n",
    "    #    print(\"\")\n",
    "        \n",
    "    #    break\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_loss = 0  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_seqs = [] # HERE\n",
    "    for inp, sequence in test_set:\n",
    "        this_seq = []\n",
    "        # Iterate over the sequencex\n",
    "        for elt in sequence:\n",
    "            ind = new_model.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        this_seq.append(new_model.char2ind[\"EOS\"])\n",
    "        all_seqs.append(torch.LongTensor(this_seq))\n",
    "            \n",
    "    all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "    #print(all_seqs)\n",
    "\n",
    "    output, logits = new_model([pair[0] for pair in test_set])\n",
    "    for index, output_guess in enumerate(output):\n",
    "        if process_output(output_guess) == test_set[index][1]:\n",
    "            correct += 1\n",
    "        #else:\n",
    "            #print(process_output(output_guess))\n",
    "            #print(test_set[index][1])\n",
    "            #print(\"\")\n",
    "        total += 1\n",
    "        \n",
    "    for index, logit in enumerate(logits):\n",
    "        if index >= len(all_seqs):\n",
    "            break\n",
    "            \n",
    "        test_loss += criterion(logit[0], all_seqs[index])\n",
    "\n",
    "    total_values = sum([len(x[1]) + 1 for x in test_set])  \n",
    "    #print(total_values)\n",
    "    test_loss /= total_values\n",
    "   \n",
    "    test_acc = correct * 1.0 / total\n",
    "                \n",
    "    \n",
    "    return test_loss, test_acc, new_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml(model, epochs, train_set, lr_inner=0.0001, lr_outer=0.001, batch_size=1, first_order=False):\n",
    "    optimizer = torch.optim.Adam(model.params(), lr=lr_outer)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        total_postupdate_test_losses = 0\n",
    "        total_test_accs = 0\n",
    "        count_postupdate_test_losses = 0\n",
    "        \n",
    "        print_total_postupdate_test_losses = 0\n",
    "        print_total_test_accs = 0\n",
    "        print_count_postupdate_test_losses = 0\n",
    "\n",
    "        for i, t in enumerate(train_set):\n",
    "            #if i % 10 == 0:\n",
    "            #    print(i)\n",
    "            \n",
    "            #print(\"MAML UPDATE\")\n",
    "            #print(model.enc_lstm.wi_bias)\n",
    "            #for param in model.params():\n",
    "            #    print(param[0])\n",
    "            #    break\n",
    "            test_loss, test_acc, new_model = fit_task(model, t, lr_inner, create_graph=not first_order)\n",
    "            #for param in model.params():\n",
    "            #    print(param[0])\n",
    "            #    break\n",
    "            #print(model.enc_lstm.wi_bias) \n",
    "            #print(new_model.enc_lstm.wi_bias)\n",
    "            #for param in new_model.params():\n",
    "            #    print(param[0])\n",
    "            #    break\n",
    "            #print(\"END MAML UPDATE\")\n",
    "            #print(\"\")\n",
    "                \n",
    "            #14/0\n",
    "            \n",
    "            total_postupdate_test_losses += test_loss\n",
    "            total_test_accs += test_acc\n",
    "            count_postupdate_test_losses += 1\n",
    "            \n",
    "            print_total_postupdate_test_losses += test_loss.item()\n",
    "            print_total_test_accs += test_acc\n",
    "            print_count_postupdate_test_losses += 1\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(i,print_total_postupdate_test_losses/print_count_postupdate_test_losses, print_total_test_accs/print_count_postupdate_test_losses)\n",
    "                print_total_postupdate_test_losses = 0\n",
    "                print_total_test_accs = 0\n",
    "                print_count_postupdate_test_losses = 0\n",
    "\n",
    "\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                \n",
    "                total_postupdate_test_losses /= count_postupdate_test_losses\n",
    "                total_postupdate_test_losses.backward(create_graph=True, retain_graph=True)\n",
    "                \n",
    "                #print(\"\")\n",
    "                #print(\"BEFORE UPDATE\")\n",
    "                #print(model.enc_lstm.wi_bias) \n",
    "                #for param in model.params():\n",
    "                #    print(param[0])\n",
    "                #    break\n",
    "                    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #print(\"AFTER UPDATE\")\n",
    "                #print(model.enc_lstm.wi_bias) \n",
    "                #for param in model.params():\n",
    "                #    print(param[0])\n",
    "                #    break\n",
    "                #print(\"\")\n",
    "                \n",
    "                total_postupdate_test_losses = 0\n",
    "                total_test_accs = 0\n",
    "                count_postupdate_test_losses = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reptile(model, epochs, train_set, lr_inner=0.0001, lr_outer=0.001, batch_size=1, first_order=False):\n",
    "    optimizer = torch.optim.Adam(model.params(), lr=lr_outer)\n",
    "    \n",
    "    name_to_param = dict(model.named_params())\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        total_postupdate_test_losses = 0\n",
    "        total_test_accs = 0\n",
    "        count_postupdate_test_losses = 0\n",
    "        \n",
    "        print_total_postupdate_test_losses = 0\n",
    "        print_total_test_accs = 0\n",
    "        print_count_postupdate_test_losses = 0\n",
    "\n",
    "        for i, t in enumerate(train_set):\n",
    "            \n",
    "            test_loss, test_acc, new_model = fit_task(model, t, lr_inner, create_graph=not first_order)\n",
    "            \n",
    "            \n",
    "            for name, param in new_model.named_params():\n",
    "                cur_grad = (name_to_param[name].data - param.data) / lr_inner\n",
    "                if name_to_param[name].grad is None:\n",
    "                    name_to_param[name].grad = V(torch.zeros(cur_grad.size()))\n",
    "                name_to_param[name].grad.data.add_(cur_grad / batch_size)\n",
    "            \n",
    "            print_total_postupdate_test_losses += test_loss.item()\n",
    "            print_total_test_accs += test_acc\n",
    "            print_count_postupdate_test_losses += 1\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(i,print_total_postupdate_test_losses/print_count_postupdate_test_losses, print_total_test_accs/print_count_postupdate_test_losses)\n",
    "                print_total_postupdate_test_losses = 0\n",
    "                print_total_test_accs = 0\n",
    "                print_count_postupdate_test_losses = 0\n",
    "\n",
    "\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                \n",
    "               \n",
    "                    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                total_postupdate_test_losses = 0\n",
    "                total_test_accs = 0\n",
    "                count_postupdate_test_losses = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_REPTILE = EncoderDecoder(6,6,6,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CV_REPTILE.named_params():\n",
    "    #print(param)\n",
    "    #break\n",
    "    pass\n",
    "\n",
    "print CV_REPTILE.enc_lstm.wi_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "_, acc, new_model = fit_task(CV_REPTILE, train_set[8], 0.001)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_output(new_model(['chpcp'])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1.783060073852539, 0.0)\n",
      "(1000, 0.9749329946637154, 0.10320930232558084)\n",
      "(2000, 0.7846943070590496, 0.16751162790697627)\n",
      "(3000, 0.6780034032464027, 0.22872093023255774)\n",
      "(4000, 0.6204485424160957, 0.2227674418604649)\n",
      "(5000, 0.5943890239596367, 0.23644186046511603)\n",
      "(6000, 0.5718764623105526, 0.22309302325581323)\n",
      "(7000, 0.565203436717391, 0.2372790697674412)\n",
      "(8000, 0.5544433343410492, 0.2547674418604648)\n",
      "(9000, 0.5509172289669514, 0.2450697674418601)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-081f74242814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV_REPTILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-136ffa17c6c4>\u001b[0m in \u001b[0;36mmaml\u001b[0;34m(model, epochs, train_set, lr_inner, lr_outer, batch_size, first_order)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#    print(param[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#    break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mfirst_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m#for param in model.params():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#    print(param[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-74618ac8bd17>\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, lr_inner, create_graph)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m#print(all_seqs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_guess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_guess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-eb133ddfe898>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence_list)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#.item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maml(CV_REPTILE, 10, train_set, batch_size=1, lr_inner=0.01, first_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.47298333048820496, 0.3023255813953488)\n",
      "(1000, 0.5191877368688583, 0.24927906976744155)\n",
      "(2000, 0.5177044985890389, 0.2578372093023253)\n",
      "(3000, 0.5197958633601666, 0.26423255813953417)\n",
      "(4000, 0.515871185451746, 0.2692093023255812)\n",
      "(5000, 0.5149109952151776, 0.2600465116279063)\n",
      "(6000, 0.5185968863070011, 0.24397674418604628)\n",
      "(7000, 0.5187369344234467, 0.25827906976744186)\n",
      "(8000, 0.5143364772498608, 0.2686511627906968)\n",
      "(9000, 0.5164067787826061, 0.24976744186046465)\n",
      "(10000, 0.5237976936101914, 0.2374651162790695)\n",
      "(11000, 0.5160904158949852, 0.25734883720930224)\n",
      "(12000, 0.5153337240815162, 0.2696511627906975)\n",
      "(13000, 0.5183304651677608, 0.25334883720930207)\n",
      "(14000, 0.516270188510418, 0.25367441860465056)\n",
      "(15000, 0.5138649631142617, 0.2741860465116277)\n",
      "(16000, 0.5170898350477219, 0.25553488372092986)\n",
      "(17000, 0.5166957820057869, 0.26272093023255777)\n",
      "(18000, 0.5226676015555859, 0.2456744186046509)\n",
      "(19000, 0.5099458540678025, 0.26951162790697625)\n",
      "(0, 0.4973919689655304, 0.32558139534883723)\n",
      "(1000, 0.5173964767754078, 0.24983720930232525)\n",
      "(2000, 0.5160998337566852, 0.25569767441860414)\n",
      "(3000, 0.5183069564700127, 0.2664883720930228)\n",
      "(4000, 0.5143068936467171, 0.27262790697674383)\n",
      "(5000, 0.5132970091104507, 0.259651162790697)\n",
      "(6000, 0.5171794278621673, 0.24493023255813917)\n",
      "(7000, 0.517574135273695, 0.2590697674418604)\n",
      "(8000, 0.5129130845963955, 0.2699767441860458)\n",
      "(9000, 0.5154444682896138, 0.25179069767441803)\n",
      "(10000, 0.5225426964461803, 0.23979069767441852)\n",
      "(11000, 0.5152477034032344, 0.2543255813953487)\n",
      "(12000, 0.5141928820610047, 0.27039534883720906)\n",
      "(13000, 0.5170781464874744, 0.2537906976744183)\n",
      "(14000, 0.5154028794169426, 0.25467441860465057)\n",
      "(15000, 0.5128689340949059, 0.2748139534883722)\n",
      "(16000, 0.5159105110168457, 0.25618604651162774)\n",
      "(17000, 0.5156703968644142, 0.264627906976744)\n",
      "(18000, 0.5218261857032775, 0.24579069767441825)\n",
      "(19000, 0.5090007432997227, 0.26986046511627815)\n",
      "(0, 0.4946061372756958, 0.32558139534883723)\n",
      "(1000, 0.5164904401302338, 0.2508837209302323)\n",
      "(2000, 0.5153091654181481, 0.2562093023255809)\n",
      "(3000, 0.5174899133145809, 0.26599999999999946)\n",
      "(4000, 0.5135570816397667, 0.2734651162790694)\n",
      "(5000, 0.5124949433803558, 0.25851162790697607)\n",
      "(6000, 0.5164249674081802, 0.24632558139534885)\n",
      "(7000, 0.5165126202404499, 0.25916279069767423)\n",
      "(8000, 0.5119482369124889, 0.2691395348837203)\n",
      "(9000, 0.5145094647705555, 0.25365116279069755)\n",
      "(10000, 0.521753375262022, 0.24181395348837179)\n",
      "(11000, 0.5145858622789383, 0.2559999999999996)\n",
      "(12000, 0.5133358783721924, 0.2703023255813953)\n",
      "(13000, 0.5163325035870076, 0.2538139534883718)\n",
      "(14000, 0.5149961462318897, 0.2543953488372087)\n",
      "(15000, 0.5121019822061061, 0.2763720930232559)\n",
      "(16000, 0.5151111281812191, 0.25744186046511613)\n",
      "(17000, 0.5150427127182484, 0.26509302325581363)\n",
      "(18000, 0.5211652875840664, 0.245395348837209)\n",
      "(19000, 0.5083179224729538, 0.2686744186046505)\n",
      "(0, 0.498058557510376, 0.32558139534883723)\n",
      "(1000, 0.5158384968042373, 0.2496744186046509)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-91ea7bcfb8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreptile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV_REPTILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-ed57a1b25ed8>\u001b[0m in \u001b[0;36mreptile\u001b[0;34m(model, epochs, train_set, lr_inner, lr_outer, batch_size, first_order)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mfirst_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-74618ac8bd17>\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, lr_inner, create_graph)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtotal_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtotal_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#print(\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reptile(CV_REPTILE, 10, train_set, batch_size=1, lr_inner=0.01, first_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.232558139535\n"
     ]
    }
   ],
   "source": [
    "_, acc, new_model = fit_task(CV_REPTILE, train_set[0], 0.001)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['VCCCV', '.V.CV.'],\n",
       "  ['CVVC', '.CV.V.'],\n",
       "  ['VCVCC', '.V.CV.'],\n",
       "  ['VVVVV', '.V.V.V.V.V.'],\n",
       "  ['VVCVC', '.V.V.CV.'],\n",
       "  ['CVCCV', '.CV.CV.'],\n",
       "  ['VVVVC', '.V.V.V.V.'],\n",
       "  ['CCVVV', '.CV.V.V.'],\n",
       "  ['V', '.V.'],\n",
       "  ['VCVVV', '.V.CV.V.V.'],\n",
       "  ['VCCC', '.V.'],\n",
       "  ['VCCVC', '.V.CV.'],\n",
       "  ['CCV', '.CV.'],\n",
       "  ['CCVCC', '.CV.'],\n",
       "  ['', ''],\n",
       "  ['CVCC', '.CV.'],\n",
       "  ['VC', '.V.'],\n",
       "  ['CC', ''],\n",
       "  ['CVCCC', '.CV.'],\n",
       "  ['VCV', '.V.CV.']],\n",
       " [['CVC', '.CV.'],\n",
       "  ['CVCVV', '.CV.CV.V.'],\n",
       "  ['VCCVV', '.V.CV.V.'],\n",
       "  ['VVC', '.V.V.'],\n",
       "  ['CVCVC', '.CV.CV.'],\n",
       "  ['VCVVC', '.V.CV.V.'],\n",
       "  ['CCVVC', '.CV.V.'],\n",
       "  ['CCCVC', '.CV.'],\n",
       "  ['VVCCC', '.V.V.'],\n",
       "  ['VVCVV', '.V.V.CV.V.'],\n",
       "  ['CCCC', ''],\n",
       "  ['CCC', ''],\n",
       "  ['VVVCC', '.V.V.V.'],\n",
       "  ['CVVCC', '.CV.V.'],\n",
       "  ['CCCV', '.CV.'],\n",
       "  ['VCCCC', '.V.'],\n",
       "  ['CVVCV', '.CV.V.CV.'],\n",
       "  ['CCVC', '.CV.'],\n",
       "  ['CVCV', '.CV.CV.'],\n",
       "  ['VV', '.V.V.'],\n",
       "  ['CCCVV', '.CV.V.'],\n",
       "  ['CVVVC', '.CV.V.V.'],\n",
       "  ['VCVCV', '.V.CV.CV.'],\n",
       "  ['CCCCV', '.CV.'],\n",
       "  ['VVCV', '.V.V.CV.'],\n",
       "  ['CCVV', '.CV.V.'],\n",
       "  ['VVVCV', '.V.V.V.CV.'],\n",
       "  ['CVVVV', '.CV.V.V.V.'],\n",
       "  ['VVVV', '.V.V.V.V.'],\n",
       "  ['VCCV', '.V.CV.'],\n",
       "  ['VCC', '.V.'],\n",
       "  ['CVVV', '.CV.V.V.'],\n",
       "  ['CVV', '.CV.V.'],\n",
       "  ['VVV', '.V.V.V.'],\n",
       "  ['C', ''],\n",
       "  ['CCVCV', '.CV.CV.'],\n",
       "  ['VCVV', '.V.CV.V.'],\n",
       "  ['VVVC', '.V.V.V.'],\n",
       "  ['VCVC', '.V.CV.'],\n",
       "  ['VVCCV', '.V.V.CV.'],\n",
       "  ['VVCC', '.V.V.'],\n",
       "  ['CCCCC', ''],\n",
       "  ['CV', '.CV.']],\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_output(new_model([''])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_REPTILEB = EncoderDecoder(35,35,35,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3.5397775173187256, 0.0)\n",
      "(1000, 1.9945365778803825, 0.01503999999999996)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3740fd78f802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV_REPTILEB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-00cba6c3fb13>\u001b[0m in \u001b[0;36mmaml\u001b[0;34m(model, epochs, train_set, lr_inner, lr_outer, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#print(\"AFTER UPDATE\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/optim/optimizer.pyc\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maml(CV_REPTILEB, 10, train_set, batch_size=1, lr_inner=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_REPTILEB.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CV_REPTILEB.state_dict(), \"CV_REPTILE.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = EncoderDecoder(35,35,35,128)\n",
    "loaded_model.load_state_dict(torch.load(\"CV_REPTILE.weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "_, acc, new_model = fit_task(loaded_model, train_set[8], 0.001)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc, new_model = fit_task(CV_REPTILEB, train_set[8], 0.001)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_output(new_model(['p'])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fit_task(model, task, lr_inner):\n",
    "    training_set = task[0]\n",
    "    test_set = task[1]\n",
    "    v_list = task[2]\n",
    "    c_list = task[3]\n",
    "    \n",
    "    new_model = EncoderDecoder(model.enc_vocab_size,model.dec_vocab_size,model.input_size,model.hidden_size)\n",
    "    new_model.copy(model, same_var=True)\n",
    "    \n",
    "    new_model.set_dict(v_list, c_list)\n",
    "    \n",
    "    loss = 0\n",
    "    criterion = nn.NLLLoss(ignore_index=0, size_average=False)\n",
    "\n",
    "    #for pair in training_set:\n",
    "    #    inp, outp = pair\n",
    "        #output, logits = new_model(inp)\n",
    "    #    output, logits = new_model([pair[0] for pair in training_set])\n",
    "\n",
    "    #    for index, logit in enumerate(logits):\n",
    "    #        if index == len(pair[1]):\n",
    "    #            loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[\"EOS\"]]))\n",
    "    #        elif index > len(pair[1]):\n",
    "    #            break\n",
    "    #        else:\n",
    "    #            loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[pair[1][index]]]))\n",
    "                \n",
    "    #print(new_model.char2ind)\n",
    "    #print(new_model.enc_vocab_size)\n",
    "    output, logits = new_model([pair[0] for pair in training_set])\n",
    "    \n",
    "    all_seqs = [] # HERE\n",
    "    for inp, sequence in training_set:\n",
    "        this_seq = []\n",
    "        # Iterate over the sequencex\n",
    "        for elt in sequence:\n",
    "            ind = new_model.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        this_seq.append(new_model.char2ind[\"EOS\"])\n",
    "        all_seqs.append(torch.LongTensor(this_seq))\n",
    "            \n",
    "    all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "    print(all_seqs)\n",
    "\n",
    "    for index, logit in enumerate(logits):\n",
    "        if index >= len(all_seqs):\n",
    "            break\n",
    "            \n",
    "        loss += criterion(logit[0], all_seqs[index])\n",
    "        \n",
    "        #if index == len(pair[1]):\n",
    "        #    loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[\"EOS\"]]))\n",
    "        #elif index > len(pair[1]):\n",
    "        #    break\n",
    "        #else:\n",
    "        #    loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[pair[1][index]]]))\n",
    "\n",
    "            \n",
    "    total_values = sum([len(x[1]) + 1 for x in training_set])  \n",
    "    loss /= total_values\n",
    "    loss.backward(create_graph=True, retain_graph=True)\n",
    "    \n",
    "    for name, param in new_model.named_params():\n",
    "        grad = param.grad\n",
    "                \n",
    "        new_model.set_param(name, param - lr_inner * grad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_loss = 0  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_seqs = [] # HERE\n",
    "    for inp, sequence in test_set:\n",
    "        this_seq = []\n",
    "        # Iterate over the sequencex\n",
    "        for elt in sequence:\n",
    "            ind = new_model.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        this_seq.append(new_model.char2ind[\"EOS\"])\n",
    "        all_seqs.append(torch.LongTensor(this_seq))\n",
    "            \n",
    "    all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "    print(all_seqs)\n",
    "\n",
    "    output, logits = new_model([pair[0] for pair in test_set])\n",
    "    for index, output_guess in enumerate(output):\n",
    "        if output_guess == test_set[index][1]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    for index, logit in enumerate(logits):\n",
    "        if index >= len(all_seqs):\n",
    "            break\n",
    "            \n",
    "        test_loss += criterion(logit[0], all_seqs[index])\n",
    "\n",
    "    total_values = sum([len(x[1]) + 1 for x in test_set])  \n",
    "    test_loss /= total_values\n",
    "    #for pair in test_set:\n",
    "        #inp, outp = pair\n",
    "        #output, logits = new_model(inp)\n",
    "        \n",
    "        #if output == outp:\n",
    "        #    correct += 1\n",
    "        #total += 1\n",
    "\n",
    "        #for index, logit in enumerate(logits):\n",
    "        #    if index == len(pair[1]):\n",
    "        #        test_loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[\"EOS\"]]))\n",
    "        #    elif index > len(pair[1]):\n",
    "        #        break\n",
    "        #    else:\n",
    "        #        test_loss += criterion(logit[0], torch.LongTensor([new_model.char2ind[pair[1][index]]]))\n",
    "                \n",
    "        #test_loss /= len(logits)\n",
    "        \n",
    "    #for index, logit in enumerate(logits):\n",
    "    #    if index >= len(all_seqs):\n",
    "    #        break\n",
    "            \n",
    "   #     test_loss += criterion(logit[0], all_seqs[index])\n",
    "\n",
    "    #test_loss /= len(test_set)\n",
    "    test_acc = correct * 1.0 / total\n",
    "                \n",
    "    # I think we save this step for the end?\n",
    "    #test_loss.backward(create_graph=True, retain_graph=True)\n",
    "    \n",
    "        \n",
    "    #return test_loss.data.cpu().numpy() ###\n",
    "    return test_loss, test_acc\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
